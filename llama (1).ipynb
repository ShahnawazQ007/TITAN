{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b20392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ü¶ô Zero-Shot Llama 3 with V1 Prompts \n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 1Ô∏è‚É£ Setup & Login\n",
    "# ------------------------------------------------\n",
    "# ‚ö†Ô∏è PASTE YOUR HUGGING FACE TOKEN HERE\n",
    "# Accept license first: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "HF_TOKEN = \"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\" \n",
    "login(token=HF_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dea44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# 1Ô∏è‚É£ Paths and Device\n",
    "# ------------------------------------------------\n",
    "# Update these paths to match your actual file locations\n",
    "test_path = r\"E:\\Shahnawaz Qureshi\\NTNU Bloom-Project\\Bloom-project\\CLO_Classification\\data\\test.csv\"\n",
    "save_dir  = r\"E:\\Shahnawaz Qureshi\\NTNU Bloom-Project\\Bloom-project\\CLO_Classification\\models\\bart_zero_shot\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Note: The pipeline handles device placement automatically, \n",
    "# but we define this variable just in case we need it for manual checks.\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device ID: {device_id} (0=GPU, -1=CPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b143960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 2Ô∏è‚É£ Load Llama 3 (4-Bit for Speed/Memory)\n",
    "# ------------------------------------------------\n",
    "print(\"üöÄ Loading Llama 3...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Text Generation Pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=15, # Keep it short (just the label)\n",
    "    do_sample=True,\n",
    "    temperature=0.01,  # Nearly deterministic\n",
    "    return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d916dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3Ô∏è‚É£ V1 Prompt Library (Your Lists)\n",
    "# ------------------------------------------------\n",
    "PROMPTS_DIRECT = [\n",
    "    \"Classify the following learning objective according to Bloom‚Äôs Taxonomy level (Remember, Understand, Apply, Analyze, Evaluate, Create): {text}\",\n",
    "    \"Identify which Bloom‚Äôs taxonomy category best represents the learning objective below: {text}\",\n",
    "    \"Determine the Bloom‚Äôs cognitive level that this learning objective belongs to: {text}\",\n",
    "]\n",
    "PROMPTS_CONTEXTUAL = [\n",
    "    \"You are an education expert evaluating learning objectives. Based on Bloom‚Äôs taxonomy, decide which cognitive level (Remember, Understand, Apply, Analyze, Evaluate, Create) best describes this statement: {text}\",\n",
    "    \"As a teacher reviewing course outcomes, identify the Bloom‚Äôs taxonomy level demonstrated in this learning objective: {text}\",\n",
    "    \"You are an educational researcher mapping outcomes to Bloom‚Äôs levels. Which category does this belong to? {text}\",\n",
    "]\n",
    "PROMPTS_VERB = [\n",
    "    \"Bloom‚Äôs taxonomy associates action verbs with cognitive levels. Determine the correct Bloom level for the following learning objective, based on its main verb and meaning: {text}\",\n",
    "    \"Analyze the main verb in this learning objective and identify which Bloom‚Äôs level it represents: {text}\",\n",
    "    \"Considering verbs like define, explain, apply, analyze, evaluate, and create, classify this learning objective: {text}\",\n",
    "]\n",
    "PROMPTS_REFLECTIVE = [\n",
    "    \"Explain briefly what type of thinking this learning objective requires (e.g., recall, comprehension, application, critical analysis, evaluation, creativity), then state its Bloom‚Äôs taxonomy level: {text}\",\n",
    "    \"Think like an instructor. Describe what mental process this objective involves, and then choose the Bloom‚Äôs taxonomy level: {text}\",\n",
    "    \"Reflect on the cognitive process behind this statement and select the Bloom‚Äôs taxonomy category (Remember, Understand, Apply, Analyze, Evaluate, Create): {text}\",\n",
    "]\n",
    "PROMPTS_DOMAIN = [\n",
    "    \"In the context of computer science education, identify the Bloom‚Äôs taxonomy level of this learning objective: {text}\",\n",
    "    \"In business and management studies, determine the Bloom‚Äôs taxonomy category that best matches this outcome: {text}\",\n",
    "    \"For engineering and technical courses, classify this learning objective according to Bloom‚Äôs taxonomy: {text}\",\n",
    "]\n",
    "\n",
    "PROMPT_LIBRARY = {\n",
    "    \"direct\": PROMPTS_DIRECT,\n",
    "    \"contextual\": PROMPTS_CONTEXTUAL,\n",
    "    \"verb_focused\": PROMPTS_VERB,\n",
    "    \"reflective\": PROMPTS_REFLECTIVE,\n",
    "    \"domain_specific\": PROMPTS_DOMAIN\n",
    "}\n",
    "\n",
    "label_cols = ['Remember', 'Understand', 'Apply', 'Analyze', 'Evaluate', 'Create']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a22cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 4Ô∏è‚É£ The Evaluation Loop\n",
    "# ------------------------------------------------\n",
    "# Note: If running on Kaggle without uploaded data, create a dummy csv to test first!\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: DATASET NOT FOUND. Update 'test_path' variable.\")\n",
    "    test_df = pd.DataFrame() # Empty to prevent crash if path is wrong\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for style, prompts_list in PROMPT_LIBRARY.items():\n",
    "    print(f\"\\n{'='*20} Testing Style: {style.upper()} {'='*20}\")\n",
    "    \n",
    "    preds, trues = [], []\n",
    "\n",
    "    for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        lo_text = str(row[\"Learning_outcome\"])\n",
    "        \n",
    "        # Ground Truth\n",
    "        gold_labels = [col for col in label_cols if row.get(col) == 1]\n",
    "        true_label = gold_labels[0] if gold_labels else \"unknown\"\n",
    "\n",
    "        # --- PREPARE PROMPT ---\n",
    "        # 1. Rotate through your V1 list\n",
    "        template = prompts_list[i % len(prompts_list)]\n",
    "        \n",
    "        # 2. Insert the text into your V1 template\n",
    "        user_content = template.format(text=lo_text)\n",
    "        \n",
    "        # 3. Format for Llama Chat\n",
    "        # We add a System instruction to force the model to be brief.\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a classifier. Answer with ONLY the category name from Bloom's Taxonomy. Do not explain.\"},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # --- GENERATE ---\n",
    "        outputs = generator(prompt)\n",
    "        generated_text = outputs[0]['generated_text'].strip()\n",
    "        \n",
    "        # --- CLEAN OUTPUT ---\n",
    "        # Llama might say \"The answer is Create.\" -> We extract \"Create\"\n",
    "        pred_label = \"unknown\"\n",
    "        for label in label_cols:\n",
    "            if label.lower() in generated_text.lower():\n",
    "                pred_label = label\n",
    "                # Prioritize exact matches (optional logic can be added here)\n",
    "                break \n",
    "        \n",
    "        preds.append(pred_label)\n",
    "        trues.append(true_label)\n",
    "\n",
    "    # --- METRICS ---\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"   >>> Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "    \n",
    "    # Detailed Table\n",
    "    report = classification_report(trues, preds, labels=label_cols, output_dict=True, zero_division=0)\n",
    "    per_level_df = pd.DataFrame(report).transpose()\n",
    "    per_level_df = per_level_df.loc[label_cols, ['precision', 'recall', 'f1-score', 'support']]\n",
    "    display(per_level_df)\n",
    "\n",
    "    results_summary.append({\n",
    "        \"Model\": \"Llama-3-8B\",\n",
    "        \"Prompt_Style\": style,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1_Score\": f1\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------\n",
    "# 5Ô∏è‚É£ Final Report\n",
    "# ------------------------------------------------\n",
    "df_results = pd.DataFrame(results_summary).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"\\nüèÜ FINAL LLAMA REPORT\")\n",
    "display(df_results)\n",
    "df_results.to_csv(os.path.join(save_dir, \"llama_v1_results.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
